# -*- coding: utf-8 -*-
"""
=============================================================================
QCR æ•°æ®åˆ†æä¸ PPT è‡ªåŠ¨ç”Ÿæˆå·¥å…·
=============================================================================

åŠŸèƒ½æ¦‚è¿°:
---------
1. è¯»å–Excelæ•°æ®å¹¶è¿›è¡Œæ•°æ®åº“å»é‡
2. æŒ‡å®šå¤„ç†æ—¶é—´å‘¨æœŸï¼ˆåŸºäºæ—¥æœŸåˆ—ï¼‰
3. è¯»å–MTMè¡¨æ ¼å¹¶æ˜ å°„æœºå‹åç§°
4. ç»Ÿè®¡å››ç§å®¡æ ¸åŸå› ï¼ˆ7å¤©æ— ç†ç”±ã€15å¤©è´¨é‡æ¢æ–°ã€180å¤©åªæ¢ä¸ä¿®ã€è´¨é‡ç»´ä¿®ï¼‰
5. ç»Ÿè®¡7å¤©æ— ç†ç”±/é7å¤©æ— ç†ç”±çš„æœºå‹åˆ†å¸ƒ
6. æŒ‰æœºå‹ç»Ÿè®¡åˆ†ç±»æè¿°è¯é¢‘æ¬¡
7. ä¸ºæ¯ä¸ªæœºå‹çš„æ‰€æœ‰åˆ†ç±»ç”Ÿæˆè¯¦ç»†æ•°æ®æ–‡ä»¶
8. ç”Ÿæˆè¡¨æ ¼+é¥¼å›¾/æŸ±çŠ¶å›¾
9. å¯é€‰ï¼šç”ŸæˆPPTæŠ¥å‘Šï¼ˆé¦–é¡µ+è¯¦æƒ…é¡µï¼‰
10. å¯é€‰ï¼šè°ƒç”¨Kimi LLMç”Ÿæˆæ™ºèƒ½åˆ†ææ‘˜è¦

ä¾èµ–å®‰è£…:
---------
pip install pandas openpyxl matplotlib pymysql sqlalchemy python-pptx requests

é…ç½®è¯´æ˜:
---------
ã€é‡è¦ã€‘Kimi APIå¯†é’¥é…ç½®ï¼ˆä½¿ç”¨--use-llmæ—¶å¿…éœ€ï¼‰ï¼š
  æ–¹å¼1ï¼ˆæ¨èï¼‰ï¼šç›´æ¥åœ¨ä»£ç ç¬¬171è¡Œä¿®æ”¹ DEFAULT_KIMI_API_KEY å¸¸é‡
  æ–¹å¼2ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ KIMI_API_KEYï¼ˆä¼˜å…ˆçº§é«˜äºä»£ç é…ç½®ï¼‰

å¯é€‰ç¯å¢ƒå˜é‡ï¼ˆå¦‚ä¸è®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰ï¼š
- KIMI_API_URL: Kimi APIåœ°å€ï¼ˆé»˜è®¤ï¼šhttps://api.moonshot.cn/v1/chat/completionsï¼‰
- KIMI_MODEL: Kimiæ¨¡å‹åç§°ï¼ˆé»˜è®¤ï¼šKimi-K2ï¼‰
- KIMI_TIMEOUT: APIè¶…æ—¶æ—¶é—´ç§’æ•°ï¼ˆé»˜è®¤ï¼š60ï¼‰
- LLM_TOP_N: LLMæ‘˜è¦TopNå‚æ•°ï¼ˆé»˜è®¤ï¼š3ï¼‰
- LLM_COVERAGE_THRESHOLD: è¦†ç›–åº¦é˜ˆå€¼ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤ï¼š80ï¼‰
- LLM_FOCUS_THRESHOLD: é‡ç‚¹æ‹¦æˆªé˜ˆå€¼ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤ï¼š10ï¼‰

åŸºæœ¬ç”¨æ³•:
---------
# ä½ç½®å‚æ•°æ–¹å¼ï¼ˆæ—§ç‰ˆå…¼å®¹ï¼‰
python Excel_Address_New_Modified.py <è¾“å…¥æ–‡ä»¶> [MTMè¡¨æ ¼] [è¾“å‡ºç›®å½•] [å¼€å§‹æ—¥æœŸ] [ç»“æŸæ—¥æœŸ]

# å‘½åå‚æ•°æ–¹å¼ï¼ˆæ¨èï¼‰
python Excel_Address_New_Modified.py <è¾“å…¥æ–‡ä»¶> --start-date <å¼€å§‹æ—¥æœŸ> --end-date <ç»“æŸæ—¥æœŸ>

å‚æ•°è¯´æ˜:
---------
å¿…éœ€å‚æ•°ï¼š
  è¾“å…¥æ–‡ä»¶                è¾“å…¥Excelæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ï¼šæŒç»­è½å…¥Dç­‰çº§ 30å¤©æœåŠ¡å•æ˜ç»†.xlsxï¼‰

å¯é€‰ä½ç½®å‚æ•°ï¼š
  MTMè¡¨æ ¼                 MTMæ˜ å°„è¡¨Excelè·¯å¾„ï¼ˆé»˜è®¤ï¼šmtm.xlsxï¼‰
  è¾“å‡ºç›®å½•                è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ï¼šoutputï¼‰
  å¼€å§‹æ—¥æœŸ                ç­›é€‰å¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DD æˆ– YYYY/MM/DDï¼‰
  ç»“æŸæ—¥æœŸ                ç­›é€‰ç»“æŸæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DD æˆ– YYYY/MM/DDï¼‰

å¯é€‰å‘½åå‚æ•°ï¼š
  --start-date            ç­›é€‰å¼€å§‹æ—¥æœŸï¼ˆä¼˜å…ˆäºä½ç½®å‚æ•°ï¼‰
  --end-date              ç­›é€‰ç»“æŸæ—¥æœŸï¼ˆä¼˜å…ˆäºä½ç½®å‚æ•°ï¼‰
  --generate-ppt          ç”ŸæˆPPTæŠ¥å‘Š
  --use-llm               è°ƒç”¨Kimi LLMç”Ÿæˆè¯¦æƒ…é¡µæ™ºèƒ½æ‘˜è¦ï¼ˆéœ€é…ç½®KIMI_API_KEYï¼‰
  --ppt-template          PPTæ¨¡æ¿æ–‡ä»¶è·¯å¾„
  --ppt-path              è¾“å‡ºPPTæ–‡ä»¶åï¼ˆé»˜è®¤ï¼šoutput/report.pptxï¼‰
  --llm-timeout           LLMè¯·æ±‚è¶…æ—¶æ—¶é—´ç§’æ•°ï¼ˆé»˜è®¤ï¼š60ï¼‰
  --llm-top-n             LLMæ‘˜è¦TopNå‚æ•°ï¼ˆé»˜è®¤ï¼š3ï¼‰
  --llm-coverage          LLMæ‘˜è¦è¦†ç›–é˜ˆå€¼ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤ï¼š80ï¼‰
  --llm-focus             LLMæ‘˜è¦é‡ç‚¹æ‹¦æˆªé˜ˆå€¼ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤ï¼š10ï¼‰
  --skip-db               è·³è¿‡æ•°æ®åº“æ£€æŸ¥å’Œå¯¼å…¥
  --test-kimi             æµ‹è¯•Kimi APIè¿é€šæ€§åé€€å‡º

ä½¿ç”¨ç¤ºä¾‹:
---------
# ç¤ºä¾‹1: åŸºæœ¬ä½¿ç”¨ï¼ˆä»…ç”ŸæˆExcelå’Œå›¾è¡¨ï¼‰
python Excel_Address_New_Modified.py "æŒç»­è½å…¥Dç­‰çº§ 30å¤©æœåŠ¡å•æ˜ç»†.xlsx"

# ç¤ºä¾‹2: æŒ‡å®šMTMæ˜ å°„è¡¨å’Œè¾“å‡ºç›®å½•
python Excel_Address_New_Modified.py "æ•°æ®.xlsx" "mtm.xlsx" "output"

# ç¤ºä¾‹3: æŒ‡å®šæ—¥æœŸèŒƒå›´
python Excel_Address_New_Modified.py "æ•°æ®.xlsx" "mtm.xlsx" "output" "2025-07-01" "2025-07-18"

# ç¤ºä¾‹4: ä½¿ç”¨å‘½åå‚æ•°æŒ‡å®šæ—¥æœŸèŒƒå›´
python Excel_Address_New_Modified.py "æ•°æ®.xlsx" --start-date "2025-07-01" --end-date "2025-07-18"

# ç¤ºä¾‹5: ç”ŸæˆPPTæŠ¥å‘Šï¼ˆä¸ä½¿ç”¨LLMï¼‰
python Excel_Address_New_Modified.py "æ•°æ®.xlsx" --generate-ppt

# ç¤ºä¾‹6: ç”ŸæˆPPTæŠ¥å‘Šå¹¶ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½æ‘˜è¦
# æ–¹å¼1: ç›´æ¥åœ¨ä»£ç ç¬¬171è¡Œé…ç½®API Keyï¼ˆæ¨èï¼‰
python Excel_Address_New_Modified.py "æ•°æ®.xlsx" --generate-ppt --use-llm

# æ–¹å¼2: é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®
export KIMI_API_KEY="your_api_key_here"  # Linux/Mac
set KIMI_API_KEY=your_api_key_here       # Windows
python Excel_Address_New_Modified.py "æ•°æ®.xlsx" --generate-ppt --use-llm

# ç¤ºä¾‹7: å®Œæ•´é…ç½®ï¼ˆè‡ªå®šä¹‰PPTè·¯å¾„ã€LLMå‚æ•°ï¼‰
python Excel_Address_New_Modified.py "æ•°æ®.xlsx" \
    --start-date "2025-07-01" \
    --end-date "2025-07-18" \
    --generate-ppt \
    --use-llm \
    --ppt-path "æˆ‘çš„æŠ¥å‘Š.pptx" \
    --llm-timeout 90 \
    --llm-top-n 5

# ç¤ºä¾‹8: è·³è¿‡æ•°æ®åº“æ£€æŸ¥ï¼ˆé¦–æ¬¡è¿è¡Œæˆ–æ— æ•°æ®åº“æ—¶ï¼‰
python Excel_Address_New_Modified.py "æ•°æ®.xlsx" --skip-db --generate-ppt

# ç¤ºä¾‹9: æµ‹è¯•Kimi APIè¿é€šæ€§ï¼ˆé…ç½®API Keyåé¦–æ¬¡ä½¿ç”¨æ—¶æ¨èï¼‰
python Excel_Address_New_Modified.py "æ•°æ®.xlsx" --test-kimi

è¾“å‡ºç»“æœ:
---------
output/
â”œâ”€â”€ å®¡æ ¸åŸå› ç»Ÿè®¡.xlsx                    # å››ç§å®¡æ ¸åŸå› ç»Ÿè®¡è¡¨
â”œâ”€â”€ 7å¤©æ— ç†ç”±_æœºå‹åˆ†å¸ƒ.xlsx               # 7å¤©æ— ç†ç”±æœºå‹åˆ†å¸ƒè¡¨
â”œâ”€â”€ é7å¤©æ— ç†ç”±_æœºå‹åˆ†å¸ƒ.xlsx             # é7å¤©æ— ç†ç”±æœºå‹åˆ†å¸ƒè¡¨
â”œâ”€â”€ å®¡æ ¸åŸå› å æ¯”.png                     # å®¡æ ¸åŸå› é¥¼å›¾
â”œâ”€â”€ 7å¤©æ— ç†ç”±_æœºå‹åˆ†å¸ƒ.png                # 7å¤©æ— ç†ç”±æœºå‹é¥¼å›¾
â”œâ”€â”€ é7å¤©æ— ç†ç”±_æœºå‹åˆ†å¸ƒ.png              # é7å¤©æ— ç†ç”±æœºå‹é¥¼å›¾
â”œâ”€â”€ åˆ†ææŠ¥å‘Š.txt                         # æ–‡æœ¬åˆ†ææŠ¥å‘Š
â”œâ”€â”€ report.pptx                          # PPTæŠ¥å‘Šï¼ˆä½¿ç”¨--generate-pptæ—¶ï¼‰
â””â”€â”€ è¯¦ç»†æ•°æ®/
    â”œâ”€â”€ 7å¤©æ— ç†ç”±/
    â”‚   â””â”€â”€ [æœºå‹åç§°]/
    â”‚       â”œâ”€â”€ [æœºå‹]_7å¤©æ— ç†ç”±_åˆ†ç±»é¢‘æ¬¡.xlsx
    â”‚       â”œâ”€â”€ [æœºå‹]_7å¤©æ— ç†ç”±_æŸ±çŠ¶å›¾.png
    â”‚       â””â”€â”€ [æœºå‹]_7å¤©æ— ç†ç”±_è¯¦ç»†æ•°æ®.xlsx
    â””â”€â”€ é7å¤©æ— ç†ç”±/
        â””â”€â”€ [æœºå‹åç§°]/
            â”œâ”€â”€ [æœºå‹]_é7å¤©æ— ç†ç”±_åˆ†ç±»é¢‘æ¬¡.xlsx
            â”œâ”€â”€ [æœºå‹]_é7å¤©æ— ç†ç”±_æŸ±çŠ¶å›¾.png
            â””â”€â”€ [æœºå‹]_é7å¤©æ— ç†ç”±_è¯¦ç»†æ•°æ®.xlsx

æ³¨æ„äº‹é¡¹:
---------
1. Excelæ–‡ä»¶é¦–åˆ—å¿…é¡»æ˜¯æ—¥æœŸåˆ—
2. æ•°æ®åº“é…ç½®é»˜è®¤ä¸ºæœ¬åœ°MySQLï¼ˆlocalhost:3306/local_qcrï¼‰
3. ä½¿ç”¨--use-llmå‰å¿…é¡»é…ç½®KIMI_API_KEYï¼ˆåœ¨ä»£ç ç¬¬171è¡Œæˆ–è®¾ç½®ç¯å¢ƒå˜é‡ï¼‰
4. ç”Ÿæˆçš„PPTé‡‡ç”¨ç©ºç™½å¸ƒå±€ï¼Œæ”¯æŒä¸­æ–‡å­—ä½“ï¼ˆå¾®è½¯é›…é»‘ï¼‰
5. å›¾ç‰‡è·¯å¾„ä¸å­˜åœ¨æ—¶ä¼šè·³è¿‡è¯¥å›¾ç‰‡ï¼Œä¸å½±å“å…¶ä»–å†…å®¹ç”Ÿæˆ
6. LLMç”Ÿæˆå¤±è´¥æ—¶ä¼šè‡ªåŠ¨é™çº§ä¸ºæœ¬åœ°æ¨¡æ¿æ–‡æœ¬
7. å»ºè®®ä¸è¦å°†åŒ…å«çœŸå®API Keyçš„ä»£ç æäº¤åˆ°å…¬å¼€çš„ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ

ä½œè€…: KC
ç‰ˆæœ¬: 2.0
æ›´æ–°æ—¥æœŸ: 2025-10
=============================================================================
"""

import argparse
import json
import os
import re
import sys
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import requests
from pptx import Presentation
from pptx.util import Inches, Pt
from sqlalchemy import create_engine

# è®¾ç½®matplotlibä¸ºéäº¤äº’å¼åç«¯ï¼ˆé¿å…tkinterç›¸å…³è­¦å‘Šï¼‰
import matplotlib
matplotlib.use('Agg')  # å¿…é¡»åœ¨å¯¼å…¥pyplotä¹‹å‰è®¾ç½®
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆWindows ç¤ºä¾‹ï¼‰ 
matplotlib.rcParams['font.family'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False


# å¤„ç†å­—ä½“è­¦å‘Š - ä½¿ç”¨æ›´å…¼å®¹çš„å­—ä½“è®¾ç½®
import warnings
warnings.filterwarnings("ignore", category=UserWarning, message=".*Glyph.*missing.*")


# -----------------------------
# APIå¯†é’¥é…ç½®ï¼ˆè¯·åœ¨æ­¤å¤„é…ç½®æ‚¨çš„Kimi API Keyï¼‰
# -----------------------------
# æ–¹å¼1: ç›´æ¥åœ¨ä»£ç ä¸­é…ç½®ï¼ˆä¸æ¨èæäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶ï¼‰
DEFAULT_KIMI_API_KEY = "sk-z4mdCQLUIpPYoMwz7CMTonTHT8rgzgiaDOkkut5AJaHgU8wh"

# æ–¹å¼2: ä»ç¯å¢ƒå˜é‡è¯»å–ï¼ˆæ¨èï¼Œç¯å¢ƒå˜é‡ä¼˜å…ˆçº§é«˜äºä»£ç é…ç½®ï¼‰
# Windows: set KIMI_API_KEY=your_api_key
# Linux/Mac: export KIMI_API_KEY=your_api_key

# æœ€ç»ˆä½¿ç”¨çš„API Keyï¼ˆä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå…¶æ¬¡ä½¿ç”¨ä»£ç é…ç½®ï¼‰
KIMI_API_KEY = os.getenv("KIMI_API_KEY", DEFAULT_KIMI_API_KEY)

# -----------------------------
# æ•°æ®åº“é…ç½®
# -----------------------------
DB_CONFIG = {
    'host': 'localhost',
    'port': 3306,
    'user': 'root',
    'password': '0929',
    'database': 'local_qcr'
}

# -----------------------------
# å¸¸é‡é…ç½®
# -----------------------------
DEFAULT_OUTPUT_DIR = "output"
DEFAULT_MTM_FILE = "mtm.xlsx"
DEFAULT_PPT_PATH = "report.pptx"
KIMI_API_URL = os.getenv("KIMI_API_URL", "https://api.moonshot.cn/v1/chat/completions")
KIMI_MODEL = os.getenv("KIMI_MODEL", "kimi-k2-0905-preview")
KIMI_TIMEOUT = int(os.getenv("KIMI_TIMEOUT", "60"))
LLM_TOP_N = int(os.getenv("LLM_TOP_N", "3"))
LLM_COVERAGE_THRESHOLD = float(os.getenv("LLM_COVERAGE_THRESHOLD", "80"))
LLM_FOCUS_THRESHOLD = float(os.getenv("LLM_FOCUS_THRESHOLD", "10"))

# PPTå­—ä½“é…ç½®ï¼ˆå¯æ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
DEFAULT_PPT_FONT = "å¾®è½¯é›…é»‘"  # å¯é€‰ï¼šå®‹ä½“ã€é»‘ä½“ã€Arialã€SimHeiç­‰
PPT_TITLE_FONT_SIZE = 28       # é¦–é¡µæ ‡é¢˜å­—å·
PPT_SUBTITLE_FONT_SIZE = 28    # è¯¦æƒ…é¡µæ ‡é¢˜å­—å·
PPT_BODY_FONT_SIZE = 14        # æ­£æ–‡å­—å·


class LLMGenerationError(Exception):
    pass


# -----------------------------
# LLMç›¸å…³è¾…åŠ©å‡½æ•°
# -----------------------------
def dataframe_to_category_rows(df: pd.DataFrame) -> List[Dict[str, str]]:
    rows = []
    for _, row in df.iterrows():
        rows.append({
            "Category": str(row.get("åˆ†ç±»", "")),
            "Count": str(row.get("æ¬¡æ•°", "")),
            "Share": str(row.get("å æ¯”", ""))
        })
    return rows


def build_prompt_payload(category_rows: List[Dict[str, str]], top_n: int, coverage_threshold: float, focus_threshold: float) -> Dict[str, str]:
    table_lines = ["åˆ†ç±»\té¢‘æ¬¡\tå æ¯”"]
    for row in category_rows:
        table_lines.append(f"{row['Category']}\t{row['Count']}\t{row['Share']}")
    table_text = "\n".join(table_lines)

    prompt = f"""# è§’è‰²
ä½ æ˜¯ä¸€åPCç”µè„‘åˆ¶é€ ä¸šçš„è´¨é‡ç®¡ç†ä¸“å®¶ä¸ç”¨æˆ·åé¦ˆåˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åœ¨ä¸¥æ ¼ä¾èµ–è¾“å…¥è¡¨æ ¼ï¼ˆåŒ…å«åˆ—ï¼šåˆ†ç±»ã€é¢‘æ¬¡ã€å æ¯”ï¼‰çš„å‰æä¸‹ï¼Œä¸å¼•å…¥å¤–éƒ¨ä¿¡æ¯ã€ä¸è‡ªè¡Œè®¡ç®—/é‡ç®—å æ¯”ï¼Œè¾“å‡ºé«˜åº¦å‡ç»ƒçš„æ ¸å¿ƒè§‚ç‚¹ä¸å¯æ‰§è¡Œå»ºè®®ï¼Œç”¨äºé—®é¢˜æ‹¦æˆªä¸åç»­å¤ç°/æ ¹å› åˆ†æã€‚

## è¾“å…¥æ•°æ®è¡¨æ ¼
{table_text}

## æŠ€èƒ½
### æŠ€èƒ½ 1: ç”Ÿæˆæ ¸å¿ƒè§‚ç‚¹
1. è¾“å…¥åŒ…å«"åˆ†ç±»""é¢‘æ¬¡""å æ¯”"çš„è¡¨æ ¼æ•°æ®ã€‚
2. å¼€ç¯‡äº¤ä»£æ ·æœ¬å¤„ç†ä¸æ€»é‡ï¼ˆå¦‚"å»é™¤æ— æ•ˆåå…± N é¡¹"ï¼‰ï¼Œå¹¶ç›´æ¥ç‚¹å Top-Nï¼š
    - é‡‡ç”¨ç´§å‡‘ä½“ä¾‹ï¼š"åˆ†ç±»å*é¢‘æ¬¡ï¼ˆå æ¯”ï¼‰"ï¼ˆç¤ºä¾‹ï¼šæ— æ³•å¼€æœº*20ï¼ˆ27.0%ï¼‰ï¼‰ã€‚
3. è¾“å‡ºå‰©ä½™åˆ†ç±»æƒ…å†µï¼š"å…¶ä½™é—®é¢˜åˆ†å¸ƒè¾ƒä¸ºåˆ†æ•£ï¼Œæ— æ˜æ˜¾é›†ä¸­æ€§"ã€‚
4. Top-Néœ€è¦æ»¡è¶³ï¼š
    - å¿…é¡»è¾“å‡ºTop1ã€‚
    - å¯¹äºTop2å’ŒTop3åˆ†åˆ«éœ€è¦å¤§äºç­‰äº15%æ‰å¯ä»¥è¢«è¾“å‡ºã€‚
    - å¯¹äºTop4å¯ä»¥ä¸è¾“å‡ºã€‚

### æŠ€èƒ½ 2: ç”Ÿæˆå¯æ‰§è¡Œå»ºè®®
1. æ ¹æ®ç”Ÿæˆçš„æ ¸å¿ƒè§‚ç‚¹ä¸­çš„Top-Nåˆ†ç±»ã€‚
2. æ˜ç¡®é‡ç‚¹æ‹¦æˆªæ¸…å•ï¼Œå¹¶ç»™å‡ºä¸‹ä¸€æ­¥ï¼š
    - ç»™å‡ºæ‹¦æˆªå»ºè®®ï¼Œä¸€èˆ¬æ‹¦æˆªTop-Nçš„æœºå‹ï¼Œå¦‚æœåˆ†ç±»é—®é¢˜çš„é¢‘æ¬¡è¾ƒå°‘å¯ä»¥ä¸æ‹¦æˆªï¼Œè¯æœ¯å‚è€ƒï¼š"å»ºè®®å¯¹æ­»æœºï¼Œæ— æ³•å¼€æœºç­‰æœºå™¨è¿›è¡Œé€€æœºæ‹¦æˆªå¤„ç†ï¼Œåšè¿›ä¸€æ­¥åˆ†æã€‚"
    - è‹¥åˆ†ç±»åå·²æ˜ç¡®æŒ‡å‘æ–¹å‘ï¼ˆå¦‚"é€‚é…å™¨-æ— æ³•å……ç”µ"ï¼‰ï¼Œå¯ç»™å‡ºæç®€æ–¹å‘æ€§çº¿ç´¢ï¼ˆä»è´¨é‡ç®¡ç†çš„è§’åº¦ï¼Œç»™å‡ºè´¨é‡é—®é¢˜çš„æ¢ç´¢æ–¹å‘ï¼Œè¦æ±‚ç²¾ç®€ä¸“ä¸šï¼‰ï¼Œé¿å…è¶Šç•Œæ¨æ–­ã€‚
    - å¯¹äºæ— ç†ç”±é€€æœºçš„åˆ†ç±»æ— éœ€ç»™å‡ºå»ºè®®ï¼Œç›´æ¥å¿½ç•¥å³å¯

## é™åˆ¶:
- ä¸å¾—è®¡ç®—/é‡ç®—å æ¯”ï¼šä¸å¾—åŸºäºé¢‘æ¬¡æ¨å¯¼å æ¯”ï¼Œä¸å¾—æ”¹å†™ä»»ä½•å•é¡¹å æ¯”ã€‚
- é›¶å¹»è§‰ï¼šä¸æ·»åŠ è¾“å…¥è¡¨æ ¼ä¹‹å¤–çš„ç±»åˆ«ã€åŸå› æˆ–æ•°æ®ã€‚
- ä¿ç•™åŸè¯ï¼šå¼•ç”¨åˆ†ç±»åæ—¶ä¿æŒä¸è¾“å…¥ä¸€è‡´ï¼ˆé™¤å»å¤šä½™ç©ºæ ¼ï¼‰ã€‚
- é£æ ¼ä¸æ•°å€¼ï¼šä¸­æ–‡ä¸ºä¸»ï¼›ç™¾åˆ†æ¯”ä»¥è¾“å…¥ä¸ºå‡†ï¼Œå±•ç¤ºåˆ°2ä½å°æ•°ï¼ˆå¦‚è¾“å…¥é2ä½å°æ•°ï¼ŒåŸæ ·è¾“å‡ºæˆ–å››èˆäº”å…¥ä½†éœ€æ³¨æ˜ï¼‰ã€‚
- è¾“å‡ºå¿…é¡»æŒ‰ç…§è§„å®šçš„æ ¼å¼å’Œè¦æ±‚è¿›è¡Œç»„ç»‡ï¼Œä¸èƒ½åç¦»æ¡†æ¶è¦æ±‚ã€‚
"""

    return {
        "role": "user",
        "content": prompt
    }


def call_kimi_api(messages: List[Dict[str, str]], timeout: int) -> str:
    api_key = KIMI_API_KEY
    if not api_key or api_key == "":
        raise LLMGenerationError("æœªé…ç½®KIMI_API_KEYï¼Œè¯·åœ¨ä»£ç ç¬¬171è¡Œæˆ–ç¯å¢ƒå˜é‡ä¸­é…ç½®API Key")

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    payload = {
        "model": KIMI_MODEL,
        "messages": messages,
        "temperature": 0.2
    }

    try:
        response = requests.post(
            KIMI_API_URL,
            headers=headers,
            data=json.dumps(payload),
            timeout=timeout
        )
    except requests.RequestException as exc:
        raise LLMGenerationError(f"Kimi API è¯·æ±‚å¼‚å¸¸: {exc}")

    if response.status_code != 200:
        raise LLMGenerationError(f"Kimi API è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")

    data = response.json()
    try:
        return data["choices"][0]["message"]["content"].strip()
    except (KeyError, IndexError) as exc:
        raise LLMGenerationError(f"Kimi API å“åº”è§£æå¤±è´¥: {exc}")


def test_kimi_connection() -> bool:
    """
    æµ‹è¯•Kimi APIè¿é€šæ€§
    å‘é€ä¸€ä¸ªç®€å•çš„é—®å€™æ¶ˆæ¯æ¥éªŒè¯APIé…ç½®æ˜¯å¦æ­£ç¡®
    
    Returns:
        bool: è¿æ¥æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
    """
    print("\n" + "="*60)
    print("ğŸ” Kimi API è¿é€šæ€§æµ‹è¯•")
    print("="*60)
    
    # æ£€æŸ¥API Keyé…ç½®
    print(f"\n1. æ£€æŸ¥API Keyé…ç½®...")
    api_key = KIMI_API_KEY
    if not api_key or api_key == "":
        print("   âŒ é”™è¯¯: æœªé…ç½®KIMI_API_KEY")
        print("   è¯·åœ¨ä»£ç ç¬¬179è¡Œä¿®æ”¹ DEFAULT_KIMI_API_KEY æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ KIMI_API_KEY")
        return False
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯ï¼ˆéšè—éƒ¨åˆ†å¯†é’¥ï¼‰
    masked_key = api_key[:10] + "..." + api_key[-8:] if len(api_key) > 18 else "***"
    print(f"   âœ“ API Key: {masked_key}")
    print(f"   âœ“ API URL: {KIMI_API_URL}")
    print(f"   âœ“ æ¨¡å‹: {KIMI_MODEL}")
    
    # å‘é€æµ‹è¯•è¯·æ±‚
    print(f"\n2. å‘é€æµ‹è¯•è¯·æ±‚...")
    test_message = {
        "role": "user",
        "content": "ä½ å¥½ï¼Œè¯·ç®€å•å›å¤'è¿æ¥æˆåŠŸ'å³å¯ã€‚"
    }
    
    try:
        response = call_kimi_api([test_message], timeout=30)
        print(f"   âœ“ è¯·æ±‚æˆåŠŸ!")
        print(f"\n3. Kimi å“åº”:")
        print(f"   {response}")
        
        print("\n" + "="*60)
        print("âœ… Kimi API è¿æ¥æµ‹è¯•æˆåŠŸï¼")
        print("="*60 + "\n")
        return True
        
    except LLMGenerationError as exc:
        print(f"   âŒ è¯·æ±‚å¤±è´¥: {exc}")
        print("\n" + "="*60)
        print("âŒ Kimi API è¿æ¥æµ‹è¯•å¤±è´¥")
        print("="*60)
        print("\nå¯èƒ½çš„åŸå› :")
        print("1. API Key ä¸æ­£ç¡®æˆ–å·²è¿‡æœŸ")
        print("2. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("3. API æœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
        print("4. API URL æˆ–æ¨¡å‹åç§°é…ç½®é”™è¯¯")
        print("\nè¯·æ£€æŸ¥é…ç½®åé‡è¯•ã€‚\n")
        return False


def generate_llm_summary(category_df: pd.DataFrame, timeout: int, top_n: int, coverage_threshold: float, focus_threshold: float) -> str:
    category_rows = dataframe_to_category_rows(category_df)
    if not category_rows:
        raise LLMGenerationError("åˆ†ç±»æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”ŸæˆLLMæ‘˜è¦")

    message = build_prompt_payload(category_rows, top_n, coverage_threshold, focus_threshold)
    return call_kimi_api([message], timeout)


def default_llm_fallback(clean_model: str, suffix: str, total_records: int) -> str:
    return (
        "æ ¸å¿ƒè§‚ç‚¹ï¼ˆHuman-Readable Core Insightsï¼‰\n"
        f"- æ ·æœ¬ï¼š{clean_model}{suffix}å…± {total_records} æ¡ï¼Œæš‚æœªèƒ½ç”Ÿæˆè‡ªåŠ¨åŒ–æ‘˜è¦ã€‚\n"
        "- æš‚æœªè·å–æ¨¡å‹ç»“è®ºï¼Œå»ºè®®äººå·¥å¤æ ¸åˆ†ç±»è¡¨ã€‚"
    )


# -----------------------------
# PPTç”Ÿæˆç›¸å…³å‡½æ•°
# -----------------------------
def add_textbox_with_content(slide, left, top, width, height, text, font_name=None, font_size=None, bold=False):
    """
    åœ¨å¹»ç¯ç‰‡ä¸­æ·»åŠ æ–‡æœ¬æ¡†
    
    Args:
        slide: å¹»ç¯ç‰‡å¯¹è±¡
        left, top, width, height: æ–‡æœ¬æ¡†ä½ç½®å’Œå°ºå¯¸ï¼ˆIncheså¯¹è±¡ï¼‰
        text: æ–‡æœ¬å†…å®¹
        font_name: å­—ä½“åç§°ï¼ˆé»˜è®¤ä½¿ç”¨DEFAULT_PPT_FONTï¼‰
        font_size: å­—å·ï¼ˆé»˜è®¤ä½¿ç”¨PPT_BODY_FONT_SIZEï¼‰
        bold: æ˜¯å¦åŠ ç²—
    """
    if font_name is None:
        font_name = DEFAULT_PPT_FONT
    if font_size is None:
        font_size = PPT_BODY_FONT_SIZE
        
    textbox = slide.shapes.add_textbox(left, top, width, height)
    text_frame = textbox.text_frame
    text_frame.clear()
    p = text_frame.paragraphs[0]
    run = p.add_run()
    run.text = text
    font = run.font
    font.name = font_name
    font.size = Pt(font_size)
    font.bold = bold
    return textbox


def add_image(slide, image_path: str, left, top, width=None, height=None):
    if image_path and Path(image_path).exists():
        slide.shapes.add_picture(image_path, left, top, width=width, height=height)


def build_homepage_slide(prs: Presentation, payload: Dict[str, Any]):
    """
    ç”Ÿæˆé¦–é¡µå¹»ç¯ç‰‡
    å¸ƒå±€ï¼šæ ‡é¢˜ï¼ˆé¡¶éƒ¨ï¼‰â†’ æ­£æ–‡ï¼ˆä¸­é—´ï¼‰â†’ å›¾è¡¨ï¼ˆåº•éƒ¨æ¨ªå‘æ’åˆ—ï¼‰
    """
    slide_layout = prs.slide_layouts[5]  # blank
    slide = prs.slides.add_slide(slide_layout)

    # ========== æ ‡é¢˜åŒºåŸŸï¼ˆé¡¶éƒ¨ï¼‰==========
    title_text = "è½å…¥Dç­‰çº§ æ•°æ®æ±‡æ€»åˆ†æç»“æœ"
    add_textbox_with_content(
        slide, 
        Inches(0.5), Inches(0.3), 
        Inches(9), Inches(0.8), 
        title_text, 
        font_size=PPT_TITLE_FONT_SIZE, 
        bold=True
    )

    # ========== æ­£æ–‡åŒºåŸŸï¼ˆä¸­é—´ï¼‰==========
    bullet_left = Inches(0.5)
    bullet_top = Inches(1.2)
    bullet_box = slide.shapes.add_textbox(bullet_left, bullet_top, Inches(9), Inches(2.5))
    frame = bullet_box.text_frame
    frame.word_wrap = True
    frame.clear()

    # æå–æ•°æ®
    start_str, end_str = payload.get("coverage_period", ("-", "-"))
    week_start, week_end = payload.get("week_range", ("-", "-"))
    product_list = payload.get("unique_models", [])
    product_text = "ã€".join(product_list[:4]) if product_list else "æš‚æ— "
    total_records = payload.get("total_records", 0)

    # Bullet 1: åŸºæœ¬ä¿¡æ¯
    p1 = frame.add_paragraph()
    p1.text = (
        f"{week_start}-{week_end}å…±æ”¶åˆ°è½å…¥Dç­‰çº§äº§å“æ•°æ®{total_records}æ¡ï¼Œè¦†ç›–å‘¨æœŸä¸º{start_str}-{end_str}ï¼Œ"
        f"äº§å“ä¸º{product_text}ç­‰ï¼Œå…±è®¡{len(product_list)}æ¬¾ã€‚"
    )
    p1.level = 0
    p1.font.name = DEFAULT_PPT_FONT
    p1.font.size = Pt(PPT_BODY_FONT_SIZE)

    # Bullet 2: å®¡æ ¸åŸå› å æ¯”
    reason_df = payload.get("reason_stats", pd.DataFrame())
    if not reason_df.empty:
        row_map = {row["å®¡æ ¸åŸå› "]: f"{row['å æ¯”']}%" for _, row in reason_df.iterrows()}
        p2 = frame.add_paragraph()
        p2.text = (
            f"å®¡æ ¸åŸå› ä¸­ï¼Œ7å¤©æ— ç†ç”±å æ¯”{row_map.get('7å¤©æ— ç†ç”±', '0%')}ï¼Œ15å¤©è´¨é‡æ¢æ–°å æ¯”{row_map.get('15å¤©è´¨é‡æ¢æ–°', '0%')}ï¼Œ"
            f"è´¨é‡ç»´ä¿®å æ¯”{row_map.get('è´¨é‡ç»´ä¿®', '0%')}ï¼Œ180å¤©åªæ¢ä¸ä¿®å æ¯”{row_map.get('180å¤©åªæ¢ä¸ä¿®', '0%')}ã€‚"
        )
        p2.level = 0
        p2.font.name = DEFAULT_PPT_FONT
        p2.font.size = Pt(PPT_BODY_FONT_SIZE)

    # Bullet 3: ä¸ƒå¤©æ— ç†ç”±æœºå‹
    model_7d_df = payload.get("model_7d_dist", pd.DataFrame())
    if not model_7d_df.empty:
        top_items = model_7d_df.head(4)
        parts = [f"{row['æœºå‹åç§°']}å æ¯”{row['å æ¯”']}%" for _, row in top_items.iterrows()]
        p3 = frame.add_paragraph()
        p3.text = "ä¸ƒå¤©æ— ç†ç”±ä¸­ï¼Œ" + "ï¼Œ".join(parts) + "ã€‚"
        p3.level = 0
        p3.font.name = DEFAULT_PPT_FONT
        p3.font.size = Pt(PPT_BODY_FONT_SIZE)

    # Bullet 4: éä¸ƒå¤©æ— ç†ç”±æœºå‹
    model_non7d_df = payload.get("model_non_7d_dist", pd.DataFrame())
    if not model_non7d_df.empty:
        top_items = model_non7d_df.head(4)
        parts = [f"{row['æœºå‹åç§°']}å æ¯”{row['å æ¯”']}%" for _, row in top_items.iterrows()]
        p4 = frame.add_paragraph()
        p4.text = "éä¸ƒå¤©æ— ç†ç”±ä¸­ï¼Œ" + "ï¼Œ".join(parts) + "ã€‚"
        p4.level = 0
        p4.font.name = DEFAULT_PPT_FONT
        p4.font.size = Pt(PPT_BODY_FONT_SIZE)

    # ========== å›¾è¡¨åŒºåŸŸï¼ˆåº•éƒ¨æ¨ªå‘æ’åˆ—ï¼‰==========
    # 3å¼ é¥¼å›¾æ¨ªå‘æ’åˆ—ï¼ŒYè½´ç»Ÿä¸€ä¸º4.2è‹±å¯¸
    chart_y = Inches(4.2)
    chart_width = Inches(2.5)
    
    # å›¾1: å®¡æ ¸åŸå› é¥¼å›¾ï¼ˆå·¦ï¼‰
    add_image(slide, payload.get("reason_chart_path"), Inches(1.0), chart_y, width=chart_width)
    
    # å›¾2: 7å¤©æœºå‹é¥¼å›¾ï¼ˆä¸­ï¼‰
    add_image(slide, payload.get("model_7d_chart_path"), Inches(4.2), chart_y, width=chart_width)
    
    # å›¾3: é7å¤©æœºå‹é¥¼å›¾ï¼ˆå³ï¼‰
    add_image(slide, payload.get("model_non7d_chart_path"), Inches(7.4), chart_y, width=chart_width)


def build_detail_slide(prs: Presentation, model_name: str, suffix: str, entry: Dict[str, Any], use_llm: bool, llm_params: Dict[str, Any]):
    """
    ç”Ÿæˆè¯¦æƒ…é¡µå¹»ç¯ç‰‡
    å¸ƒå±€ï¼šæ ‡é¢˜ï¼ˆé¡¶éƒ¨ï¼‰â†’ æ­£æ–‡ï¼ˆä¸­é—´ï¼‰â†’ å›¾è¡¨ï¼ˆåº•éƒ¨å±…ä¸­ï¼‰
    """
    slide_layout = prs.slide_layouts[5]
    slide = prs.slides.add_slide(slide_layout)

    # ========== æ ‡é¢˜åŒºåŸŸï¼ˆé¡¶éƒ¨ï¼‰==========
    title = f"{model_name} {suffix}åˆ†ç±»"
    add_textbox_with_content(
        slide, 
        Inches(0.5), Inches(0.3), 
        Inches(9), Inches(0.8), 
        title, 
        font_size=PPT_SUBTITLE_FONT_SIZE, 
        bold=True
    )

    # ========== æ­£æ–‡åŒºåŸŸï¼ˆä¸­é—´ï¼‰==========
    # ç”ŸæˆLLMå†…å®¹æˆ–ä½¿ç”¨fallbackï¼Œå¹¶æ‰“å°è¿›åº¦
    print(f"â†’ å¼€å§‹ç”Ÿæˆè¯¦æƒ…é¡µï¼šæœºå‹='{model_name}', ç±»å‹='{suffix}'")
    text_content = ""
    if use_llm:
        try:
            print(f"   è°ƒç”¨Kimiç”Ÿæˆè§‚ç‚¹ä¸­... æœºå‹='{model_name}', ç±»å‹='{suffix}'")
            text_content = generate_llm_summary(
                entry.get("category_df", pd.DataFrame()),
                timeout=llm_params["timeout"],
                top_n=llm_params["top_n"],
                coverage_threshold=llm_params["coverage"],
                focus_threshold=llm_params["focus"]
            )
            print(f"   âœ“ Kimiè¿”å›è§‚ç‚¹ï¼ˆ{model_name}-{suffix}ï¼‰ï¼š\n{text_content}\n")
        except LLMGenerationError as exc:
            print(f"   âœ— LLMç”Ÿæˆå¤±è´¥[{model_name}-{suffix}]: {exc}")
            text_content = default_llm_fallback(entry.get("clean_model", ""), suffix, entry.get("total_records", 0))
            print(f"   â†’ ä½¿ç”¨æœ¬åœ°æ¨¡æ¿è§‚ç‚¹ï¼ˆ{model_name}-{suffix}ï¼‰ï¼š\n{text_content}\n")
    else:
        text_content = default_llm_fallback(entry.get("clean_model", ""), suffix, entry.get("total_records", 0))
        print(f"   ï¼ˆæœªå¯ç”¨LLMï¼‰æœ¬åœ°æ¨¡æ¿è§‚ç‚¹ï¼ˆ{model_name}-{suffix}ï¼‰ï¼š\n{text_content}\n")

    # æ·»åŠ æ­£æ–‡æ–‡æœ¬æ¡†ï¼ˆä¸­é—´åŒºåŸŸï¼‰
    add_textbox_with_content(
        slide, 
        Inches(0.5), Inches(1.2), 
        Inches(9), Inches(2.8), 
        text_content,
        font_size=PPT_BODY_FONT_SIZE
    )

    # ========== å›¾è¡¨åŒºåŸŸï¼ˆåº•éƒ¨å±…ä¸­ï¼‰==========
    # æŸ±çŠ¶å›¾å±…ä¸­æ˜¾ç¤º
    chart_width = Inches(4.5)
    chart_left = Inches(2.75)  # (10 - 4.5) / 2 = 2.75 å®ç°å±…ä¸­
    chart_top = Inches(4.5)
    
    add_image(slide, entry.get("chart_path"), chart_left, chart_top, width=chart_width)
    print(f"â† å®Œæˆï¼šæœºå‹='{model_name}', ç±»å‹='{suffix}' çš„è¯¦æƒ…é¡µ\n")


def generate_ppt(summary_payload: Dict[str, Any], output_path: Path, template_path: Optional[Path], use_llm: bool, llm_params: Dict[str, Any]):
    if template_path and template_path.exists():
        prs = Presentation(template_path)
    else:
        prs = Presentation()

    print("\n===== å¼€å§‹ç”ŸæˆPPT =====")
    print("ç”Ÿæˆé¦–é¡µ...")
    build_homepage_slide(prs, summary_payload)

    model_details = summary_payload.get("model_details", {})
    for model_name, entries in model_details.items():
        for suffix in ["7å¤©æ— ç†ç”±", "é7å¤©æ— ç†ç”±"]:
            entry = entries.get(suffix)
            if not entry:
                continue
            print(f"å‡†å¤‡ç”Ÿæˆï¼šæœºå‹='{model_name}', ç±»å‹='{suffix}' çš„è¯¦æƒ…é¡µ...")
            build_detail_slide(prs, model_name, suffix, entry, use_llm, llm_params)

    prs.save(str(output_path))
    print(f"PPTæŠ¥å‘Šå·²ç”Ÿæˆï¼š{output_path}")
    print("===== ç”Ÿæˆå®Œæˆ =====\n")


def parse_arguments():
    parser = argparse.ArgumentParser(
        description="å¤„ç†QCRæ•°æ®å¹¶ç”Ÿæˆåˆ†ææŠ¥å‘Šã€å›¾è¡¨åŠå¯é€‰PPT"
    )
    parser.add_argument("input_file", help="è¾“å…¥Excelæ–‡ä»¶è·¯å¾„")
    parser.add_argument("mtm_file", nargs="?", default=DEFAULT_MTM_FILE, help="MTMæ˜ å°„è¡¨Excelè·¯å¾„")
    parser.add_argument("output_dir", nargs="?", default=DEFAULT_OUTPUT_DIR, help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("start_date_arg", nargs="?", default=None, help="å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("end_date_arg", nargs="?", default=None, help="ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")

    parser.add_argument("--start-date", dest="start_date_opt", help="å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--end-date", dest="end_date_opt", help="ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--generate-ppt", action="store_true", help="ç”ŸæˆPPTæŠ¥å‘Š")
    parser.add_argument("--use-llm", action="store_true", help="è°ƒç”¨Kimi LLMç”Ÿæˆè¯¦æƒ…é¡µæ‘˜è¦")
    parser.add_argument("--ppt-template", dest="ppt_template", default=None, help="PPTæ¨¡æ¿æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--ppt-path", dest="ppt_path", default=None, help="è¾“å‡ºPPTæ–‡ä»¶å")
    parser.add_argument("--llm-timeout", dest="llm_timeout", type=int, default=KIMI_TIMEOUT, help="LLMè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)")
    parser.add_argument("--llm-top-n", dest="llm_top_n", type=int, default=LLM_TOP_N, help="LLMæ‘˜è¦TopNå‚æ•°")
    parser.add_argument(
        "--llm-coverage", dest="llm_coverage", type=float, default=LLM_COVERAGE_THRESHOLD,
        help="LLMæ‘˜è¦è¦†ç›–é˜ˆå€¼(%)"
    )
    parser.add_argument(
        "--llm-focus", dest="llm_focus", type=float, default=LLM_FOCUS_THRESHOLD,
        help="LLMæ‘˜è¦é‡ç‚¹æ‹¦æˆªé˜ˆå€¼(%)"
    )
    parser.add_argument("--skip-db", action="store_true", help="è·³è¿‡æ•°æ®åº“æ£€æŸ¥å’Œå¯¼å…¥")
    parser.add_argument("--test-kimi", action="store_true", help="æµ‹è¯•Kimi APIè¿é€šæ€§åé€€å‡º")

    return parser.parse_args()

# -----------------------------
# å·¥å…·å‡½æ•°ï¼šæ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
# -----------------------------
def sanitize_filename(filename):
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
    # Windowséæ³•å­—ç¬¦ï¼š<>:"/\|?*
    illegal_chars = r'[<>:\"/\\|?*]'
    # æ›¿æ¢ä¸ºç©ºæ ¼
    filename = re.sub(illegal_chars, ' ', filename)
    # å»é™¤å‰åç©ºæ ¼
    filename = filename.strip()
    # é™åˆ¶é•¿åº¦
    if len(filename) > 200:
        filename = filename[:200]
    return filename

# -----------------------------
# æ•°æ®åº“å·¥å…·å‡½æ•°
# -----------------------------
def check_and_import_new_data(df):
    """æ£€æŸ¥æ•°æ®åº“ä¸­ä¸å­˜åœ¨çš„æœåŠ¡å•å·å¹¶å¯¼å…¥æ–°æ•°æ®"""
    try:
        print("å¼€å§‹è¿æ¥æ•°æ®åº“...")
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        connection_string = (
            f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@"
            f"{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}"
        )
        engine = create_engine(connection_string)
        
        # æ£€æŸ¥æ•°æ®æ¡†ä¸­æ˜¯å¦æœ‰æœåŠ¡å•å·åˆ—
        service_order_column = None
        for col in df.columns:
            if 'æœåŠ¡å•å·' in str(col):
                service_order_column = col
                break
        
        if service_order_column is None:
            print("è­¦å‘Šï¼šæœªæ‰¾åˆ°'æœåŠ¡å•å·'åˆ—ï¼Œè·³è¿‡æ•°æ®åº“æ£€æŸ¥")
            return df
        
        # è·å–å½“å‰æ•°æ®ä¸­çš„æœåŠ¡å•å·
        current_service_orders = df[service_order_column].dropna().astype(str).tolist()
        print(f"å½“å‰æ•°æ®åŒ…å« {len(current_service_orders)} ä¸ªæœåŠ¡å•å·")
        
        # æŸ¥è¯¢æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„æœåŠ¡å•å·
        try:
            # å…ˆæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            table_exists = pd.read_sql(
                "SELECT COUNT(*) as count FROM information_schema.tables WHERE table_schema = %s AND table_name = 'qcr_data'", 
                engine, 
                params=(DB_CONFIG['database'],)
            )['count'].iloc[0] > 0
            
            if table_exists:
                existing_service_orders = pd.read_sql(
                    "SELECT service_order_id FROM qcr_data", 
                    engine
                )['service_order_id'].astype(str).tolist()
                print(f"æ•°æ®åº“ä¸­å·²å­˜åœ¨ {len(existing_service_orders)} ä¸ªæœåŠ¡å•å·")
            else:
                print("æ•°æ®åº“è¡¨qcr_dataä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°è¡¨")
                existing_service_orders = []
        except Exception as e:
            print(f"æŸ¥è¯¢æ•°æ®åº“å¤±è´¥ï¼Œå‡è®¾æ•°æ®åº“ä¸ºç©º: {e}")
            existing_service_orders = []
        
        # ç­›é€‰å‡ºæ•°æ®åº“ä¸­ä¸å­˜åœ¨çš„æ–°æœåŠ¡å•å·
        new_service_orders = [
            order for order in current_service_orders 
            if order not in existing_service_orders
        ]
        print(f"æ–°æœåŠ¡å•å·æ•°é‡: {len(new_service_orders)}")
        
        # ç­›é€‰æ–°æ•°æ®
        df_new = df[df[service_order_column].astype(str).isin(new_service_orders)].copy()
        
        if len(df_new) == 0:
            print("æ²¡æœ‰æ–°æ•°æ®éœ€è¦å¯¼å…¥å’Œåˆ†æ")
            return df_new
        
        # å‡†å¤‡å¯¼å…¥æ•°æ®åº“çš„æ•°æ®
        df_to_import = df_new.copy()
        
        # é‡å‘½ååˆ—ä»¥åŒ¹é…æ•°æ®åº“å­—æ®µ
        column_mapping = {}
        for col in df_to_import.columns:
            if 'æœåŠ¡å•å·' in str(col):
                column_mapping[col] = 'service_order_id'
            elif 'æ—¥æœŸ' in str(col):
                column_mapping[col] = 'date'
            elif 'è®¢å•å·' in str(col):
                column_mapping[col] = 'order_id'
            elif 'é—®é¢˜æè¿°' in str(col):
                column_mapping[col] = 'issue_description'
            elif 'SKU' in str(col):
                column_mapping[col] = 'sku'
            elif 'SNç¼–ç ' in str(col):
                column_mapping[col] = 'sn_code'
            elif 'å®¢æˆ·è´¦å·' in str(col) or 'å®¢æˆ·è´¦æˆ·' in str(col):
                column_mapping[col] = 'customer_account'
            elif 'å•†å“åç§°' in str(col):
                column_mapping[col] = 'product_name'
            elif 'MTM' in str(col):
                column_mapping[col] = 'mtm'
            elif 'å®¡æ ¸åŸå› ' in str(col):
                column_mapping[col] = 'audit_reason'
            elif 'é—®é¢˜åˆ†ç±»' in str(col) and 'ä¸€' not in str(col):
                column_mapping[col] = 'issue_category'
            elif 'åˆ†ç±»' in str(col) and 'é—®é¢˜' not in str(col):
                column_mapping[col] = 'category'
        
        # åº”ç”¨åˆ—æ˜ å°„
        df_to_import = df_to_import.rename(columns=column_mapping)
        
        # ç¡®ä¿å¿…éœ€çš„åˆ—å­˜åœ¨
        required_db_columns = [
            'service_order_id', 'date', 'order_id', 'issue_description', 
            'sku', 'sn_code', 'customer_account', 'product_name', 
            'mtm', 'audit_reason', 'issue_category', 'category'
        ]
        
        for col in required_db_columns:
            if col not in df_to_import.columns:
                df_to_import[col] = ''
        
        # æ•°æ®ç±»å‹è½¬æ¢å’Œæ¸…æ´—
        # å¤„ç†æ—¥æœŸ
        if 'date' in df_to_import.columns:
            df_to_import['date'] = pd.to_datetime(df_to_import['date'], errors='coerce').dt.strftime('%Y-%m-%d')
            df_to_import['date'] = df_to_import['date'].fillna('1900-01-01')
        
        # å¤„ç†æ•°å€¼åˆ—
        numeric_columns = ['service_order_id', 'order_id', 'sku']
        for col in numeric_columns:
            if col in df_to_import.columns:
                df_to_import[col] = pd.to_numeric(df_to_import[col], errors='coerce').astype('Int64')
        
        # å¤„ç†å­—ç¬¦ä¸²åˆ—
        string_columns = [
            'issue_description', 'sn_code', 'customer_account', 
            'product_name', 'mtm', 'audit_reason', 
            'issue_category', 'category'
        ]
        for col in string_columns:
            if col in df_to_import.columns:
                df_to_import[col] = df_to_import[col].fillna('').astype(str).str.strip()
                # å­—ç¬¦ä¸²é•¿åº¦é™åˆ¶
                max_lengths = {
                    'issue_description': 500,
                    'sn_code': 100,
                    'customer_account': 100,
                    'product_name': 200,
                    'mtm': 50,
                    'audit_reason': 100,
                    'issue_category': 100,
                    'category': 100
                }
                if col in max_lengths:
                    df_to_import[col] = df_to_import[col].str[:max_lengths[col]]
        
        # åˆ é™¤å¿…éœ€å­—æ®µä¸ºç©ºçš„è¡Œ
        if 'service_order_id' in df_to_import.columns:
            df_to_import = df_to_import.dropna(subset=['service_order_id'])
        
        # åªé€‰æ‹©æ•°æ®åº“éœ€è¦çš„åˆ—
        df_to_import = df_to_import[required_db_columns]
        
        # å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“
        if len(df_to_import) > 0:
            try:
                df_to_import.to_sql(
                    'qcr_data', 
                    engine, 
                    if_exists='append', 
                    index=False,
                    method='multi'
                )
                print(f"æˆåŠŸå¯¼å…¥ {len(df_to_import)} æ¡æ–°è®°å½•åˆ°æ•°æ®åº“")
            except Exception as e:
                print(f"å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")
                print("å°†ç»§ç»­åˆ†æå½“å‰æ•°æ®ï¼Œä½†æ–°æ•°æ®ä¸ä¼šä¿å­˜åˆ°æ•°æ®åº“")
        
        # è¿”å›æ–°æ•°æ®ç”¨äºåç»­åˆ†æ
        return df_new
        
    except Exception as e:
        print(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
        print("å°†ç»§ç»­åˆ†æåŸå§‹æ•°æ®ï¼Œè·³è¿‡æ•°æ®åº“æ£€æŸ¥å’Œå¯¼å…¥")
        return df

# -----------------------------
# 1. å‘½ä»¤è¡Œå‚æ•°å¤„ç†
# -----------------------------
args = parse_arguments()

# -----------------------------
# Kimi API è¿é€šæ€§æµ‹è¯•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
# -----------------------------
if args.test_kimi:
    test_kimi_connection()
    sys.exit(0)  # æµ‹è¯•å®Œæˆåé€€å‡º

file_path = Path(args.input_file)
if not file_path.exists():
    print(f"é”™è¯¯: æ–‡ä»¶ '{file_path}' ä¸å­˜åœ¨")
    sys.exit(1)

mtm_file_path = Path(args.mtm_file)
if not mtm_file_path.exists():
    print(f"è­¦å‘Š: MTMè¡¨æ ¼æ–‡ä»¶ '{mtm_file_path}' ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨åŸå§‹MTMå€¼")
    use_mtm_mapping = False
else:
    use_mtm_mapping = True

out_dir = Path(args.output_dir)
out_dir.mkdir(parents=True, exist_ok=True)
sheet_name = 0  # é»˜è®¤ç¬¬ä¸€å¼ è¡¨

# -----------------------------
# æ—¥æœŸè§£æå‡½æ•°
# -----------------------------
def parse_date(date_str):
    """å°è¯•è§£æå¤šç§æ—¥æœŸæ ¼å¼"""
    for fmt in ("%Y-%m-%d", "%Y/%m/%d"):
        try:
            return datetime.strptime(date_str, fmt).date()
        except ValueError:
            continue
    raise ValueError(f"æ— æ³•è§£ææ—¥æœŸ: {date_str}ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æˆ– YYYY/MM/DD æ ¼å¼")


def format_percentage(value: float, decimals: int = 1) -> str:
    return f"{value:.{decimals}f}%"


def parse_percentage(value) -> Optional[float]:
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return float(value)
    value_str = str(value).strip().replace('%', '')
    if not value_str:
        return None
    try:
        return float(value_str)
    except ValueError:
        return None


def get_week_workday_range(reference_date: Optional[date] = None) -> Tuple[str, str]:
    today = reference_date if reference_date else datetime.today().date()
    monday = today - timedelta(days=today.weekday())
    friday = monday + timedelta(days=4)
    return monday.strftime("%Y/%m/%d"), friday.strftime("%Y/%m/%d")


def determine_coverage_range(df: pd.DataFrame, date_column: str, start_date: Optional[date], end_date: Optional[date]) -> Tuple[str, str]:
    if df.empty:
        return ("-", "-")
    
    # ç¡®ä¿æ—¥æœŸç±»å‹ä¸€è‡´
    actual_start = start_date if start_date else df[date_column].min()
    actual_end = end_date if end_date else df[date_column].max()
    
    # å°† pandas Timestamp è½¬æ¢ä¸º date å¯¹è±¡
    if hasattr(actual_start, 'date') and callable(actual_start.date):
        actual_start = actual_start.date()
    if hasattr(actual_end, 'date') and callable(actual_end.date):
        actual_end = actual_end.date()
    
    return actual_start.strftime("%Y/%m/%d"), actual_end.strftime("%Y/%m/%d")


def join_top_items(items: List[str], limit: int) -> str:
    filtered = [str(item) for item in items if str(item)]
    return "ã€".join(filtered[:limit])

# -----------------------------
# è·å–æ—¥æœŸèŒƒå›´å‚æ•°
# -----------------------------
start_date = None
end_date = None

def resolve_date(raw_date):
    if not raw_date:
        return None
    try:
        return parse_date(raw_date)
    except ValueError as exc:
        print(f"è­¦å‘Š: {exc}ï¼Œå°†å¤„ç†æ‰€æœ‰æ•°æ®")
        return None

start_date = resolve_date(args.start_date_opt or args.start_date_arg)
end_date = resolve_date(args.end_date_opt or args.end_date_arg)

# -----------------------------
# 2. è¯»å–æ•°æ®
# -----------------------------
df = pd.read_excel(file_path, sheet_name=sheet_name)

# å‡è®¾ç¬¬ä¸€åˆ—æ˜¯æ—¥æœŸåˆ—ï¼Œè½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼
date_column = df.columns[0]
df[date_column] = pd.to_datetime(df[date_column]).dt.date

# æ ¹æ®æ—¥æœŸèŒƒå›´ç­›é€‰æ•°æ®
if start_date and end_date:
    mask = (df[date_column] >= start_date) & (df[date_column] <= end_date)
    df = df[mask]
    print(f"å·²ç­›é€‰ {start_date} åˆ° {end_date} çš„æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
elif start_date:
    mask = df[date_column] >= start_date
    df = df[mask]
    print(f"å·²ç­›é€‰ {start_date} ä¹‹åçš„æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
elif end_date:
    mask = df[date_column] <= end_date
    df = df[mask]
    print(f"å·²ç­›é€‰ {end_date} ä¹‹å‰çš„æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")

# -----------------------------
# æ•°æ®åº“æ£€æŸ¥å’Œå¯¼å…¥æ–°æ•°æ®
# -----------------------------
if args.skip_db:
    print("å·²è·³è¿‡æ•°æ®åº“æ£€æŸ¥ä¸å¯¼å…¥")
else:
    print("\nå¼€å§‹æ£€æŸ¥æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„æœåŠ¡å•å·...")
    df = check_and_import_new_data(df)
    print(f"æ•°æ®åº“æ£€æŸ¥åï¼Œå‰©ä½™ {len(df)} æ¡æ–°è®°å½•éœ€è¦åˆ†æ\n")

# -----------------------------
# 3. å»é‡MTM
# -----------------------------
# original_count = len(df)
# df = df.drop_duplicates(subset=['MTM'])
# print(f"å·²å»é‡MTMï¼Œä» {original_count} æ¡è®°å½•å‡å°‘åˆ° {len(df)} æ¡è®°å½•")

# -----------------------------
# 4. è¯»å–MTMæ˜ å°„è¡¨
# -----------------------------
if use_mtm_mapping:
    mtm_df = pd.read_excel(mtm_file_path, sheet_name=sheet_name, header=None)
    mtm_df.columns = ['MTM', 'æœºå‹åç§°']
    mtm_mapping = dict(zip(mtm_df['MTM'], mtm_df['æœºå‹åç§°']))
    
    # æ˜ å°„MTMåˆ°æœºå‹åç§°
    df['æœºå‹åç§°'] = df['MTM'].map(mtm_mapping).fillna(df['MTM'])
else:
    # å¦‚æœæ²¡æœ‰MTMæ˜ å°„è¡¨ï¼Œä½¿ç”¨åŸå§‹MTMå€¼ä½œä¸ºæœºå‹åç§°
    df['æœºå‹åç§°'] = df['MTM']

# -----------------------------
# 5. é¢„è®¡ç®—å¸¸ç”¨æ¡ä»¶
# -----------------------------
cond_7d = df["å®¡æ ¸åŸå› "] == "7å¤©æ— ç†ç”±"
cond_non_7d = df["å®¡æ ¸åŸå› "].isin(["15å¤©è´¨é‡æ¢æ–°", "180å¤©åªæ¢ä¸ä¿®", "è´¨é‡ç»´ä¿®"])

# ç¼“å­˜ä¸­é—´ç»“æœ
df_7d = df[cond_7d].copy()
df_non_7d = df[cond_non_7d].copy()

# åˆ›å»ºæ–‡ä»¶å¤¹ç»“æ„
detailed_dir_7d = out_dir / "è¯¦ç»†æ•°æ®" / "7å¤©æ— ç†ç”±"
detailed_dir_non7d = out_dir / "è¯¦ç»†æ•°æ®" / "é7å¤©æ— ç†ç”±"
detailed_dir_7d.mkdir(parents=True, exist_ok=True)
detailed_dir_non7d.mkdir(parents=True, exist_ok=True)

# -----------------------------
# 6. ç»Ÿè®¡å››ç§å®¡æ ¸åŸå› 
# -----------------------------
reasons = ["15å¤©è´¨é‡æ¢æ–°", "180å¤©åªæ¢ä¸ä¿®", "7å¤©æ— ç†ç”±", "è´¨é‡ç»´ä¿®"]
counts = {r: int((df["å®¡æ ¸åŸå› "] == r).sum()) for r in reasons}

summary_df = pd.DataFrame(list(counts.items()), columns=["å®¡æ ¸åŸå› ", "æ•°é‡"])
total_reason_count = summary_df["æ•°é‡"].sum()
summary_df["å æ¯”"] = (summary_df["æ•°é‡"] / total_reason_count * 100).round(2)
summary_df.to_excel(out_dir / "å®¡æ ¸åŸå› ç»Ÿè®¡.xlsx", index=False)

plt.figure(figsize=(6, 6))
plt.pie(summary_df["æ•°é‡"], labels=summary_df["å®¡æ ¸åŸå› "], autopct="%1.1f%%")
plt.title("å®¡æ ¸åŸå› å æ¯”")
plt.tight_layout()
reason_chart_path = out_dir / "å®¡æ ¸åŸå› å æ¯”.png"
plt.savefig(reason_chart_path)
plt.close()

# -----------------------------
# 7. 7å¤©æ— ç†ç”±æœºå‹åˆ†å¸ƒ
# -----------------------------
model_7d_dist = pd.DataFrame()
model_7d_chart_path = None
if len(df_7d) > 0:
    model_7d_dist = (
        df_7d["æœºå‹åç§°"]
        .value_counts()
        .rename_axis("æœºå‹åç§°")
        .reset_index(name="æ•°é‡")
        .assign(å æ¯”=lambda x: (x["æ•°é‡"] / x["æ•°é‡"].sum() * 100).round(1))
    )
    model_7d_dist.to_excel(out_dir / "7å¤©æ— ç†ç”±_æœºå‹åˆ†å¸ƒ.xlsx", index=False)

    plt.figure(figsize=(8, 8))
    plt.pie(model_7d_dist["æ•°é‡"], labels=model_7d_dist["æœºå‹åç§°"], autopct="%1.1f%%")
    plt.title("7å¤©æ— ç†ç”± - æœºå‹åˆ†å¸ƒ")
    plt.tight_layout()
    model_7d_chart_path = out_dir / "7å¤©æ— ç†ç”±_æœºå‹åˆ†å¸ƒ.png"
    plt.savefig(model_7d_chart_path)
    plt.close()
else:
    print("è­¦å‘Šï¼š7å¤©æ— ç†ç”±æ•°æ®ä¸ºç©º")

# -----------------------------
# 8. é7å¤©æ— ç†ç”±æœºå‹åˆ†å¸ƒ
# -----------------------------
model_non_7d_dist = pd.DataFrame()
model_non_7d_chart_path = None
if len(df_non_7d) > 0:
    model_non_7d_dist = (
        df_non_7d["æœºå‹åç§°"]
        .value_counts()
        .rename_axis("æœºå‹åç§°")
        .reset_index(name="æ•°é‡")
        .assign(å æ¯”=lambda x: (x["æ•°é‡"] / x["æ•°é‡"].sum() * 100).round(1))
    )
    model_non_7d_dist.to_excel(out_dir / "é7å¤©æ— ç†ç”±_æœºå‹åˆ†å¸ƒ.xlsx", index=False)

    plt.figure(figsize=(8, 8))
    plt.pie(model_non_7d_dist["æ•°é‡"], labels=model_non_7d_dist["æœºå‹åç§°"], autopct="%1.1f%%")
    plt.title("é7å¤©æ— ç†ç”± - æœºå‹åˆ†å¸ƒ")
    plt.tight_layout()
    model_non_7d_chart_path = out_dir / "é7å¤©æ— ç†ç”±_æœºå‹åˆ†å¸ƒ.png"
    plt.savefig(model_non_7d_chart_path)
    plt.close()
else:
    print("è­¦å‘Šï¼šé7å¤©æ— ç†ç”±æ•°æ®ä¸ºç©º")

# -----------------------------
# 9. æŒ‰æœºå‹ç»Ÿè®¡åˆ†ç±»æè¿°è¯é¢‘æ¬¡
# -----------------------------
def build_model_issue_table(df_sub, suffix, detailed_dir):
    """ä¸ºæ¯ä¸ªæœºå‹è®¡ç®—'åˆ†ç±»'æè¿°è¯é¢‘æ¬¡ï¼Œè¾“å‡ºexcelå’ŒæŸ±çŠ¶å›¾ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†æ•°æ®æ–‡ä»¶"""
    if len(df_sub) == 0:
        print(f"è­¦å‘Šï¼š{suffix}æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡æœºå‹åˆ†æ")
        return []

    summaries = []
        
    # é7å¤©æ— ç†ç”±æ•°æ®ï¼šè¿‡æ»¤æ‰é—®é¢˜æè¿°ä¸ºç©ºçš„è¡Œ
    if suffix == "é7å¤©æ— ç†ç”±":
        # æ£€æŸ¥é—®é¢˜æè¿°åˆ—æ˜¯å¦å­˜åœ¨
        if "é—®é¢˜æè¿°" in df_sub.columns:
            # è¿‡æ»¤æ‰é—®é¢˜æè¿°ä¸ºç©ºçš„è¡Œ
            df_sub = df_sub[df_sub["é—®é¢˜æè¿°"].notna() & (df_sub["é—®é¢˜æè¿°"] != "")]
            print(f"å·²è¿‡æ»¤ç©ºé—®é¢˜æè¿°è¡Œï¼Œå‰©ä½™ {len(df_sub)} æ¡è®°å½•")
        else:
            print("è­¦å‘Šï¼šæœªæ‰¾åˆ°'é—®é¢˜æè¿°'åˆ—ï¼Œæ— æ³•è¿‡æ»¤ç©ºå€¼")
    
    for model in df_sub["æœºå‹åç§°"].unique():
        # æ¸…ç†æœºå‹åç§°ç”¨äºæ–‡ä»¶å¤¹å’Œæ–‡ä»¶å
        clean_model = sanitize_filename(str(model))
        
        # åˆ›å»ºæœºå‹æ–‡ä»¶å¤¹
        model_dir = detailed_dir / clean_model
        model_dir.mkdir(parents=True, exist_ok=True)
        
        # è·å–è¯¥æœºå‹çš„æ‰€æœ‰æ•°æ®
        model_data = df_sub[df_sub["æœºå‹åç§°"] == model].copy()
        
        # ç»Ÿè®¡åˆ†ç±»é¢‘æ¬¡
        sub = (
            model_data["åˆ†ç±»"]
            .value_counts()
            .rename_axis("åˆ†ç±»")
            .reset_index(name="æ¬¡æ•°")
        )

        if "æ¬¡æ•°" in sub.columns and sub["æ¬¡æ•°"].sum() > 0:
            sub["å æ¯”"] = (sub["æ¬¡æ•°"] / sub["æ¬¡æ•°"].sum() * 100).round(1)
        else:
            sub["å æ¯”"] = 0
        
        # ä¿å­˜é¢‘æ¬¡ç»Ÿè®¡
        freq_filename = f"{clean_model}_{suffix}_åˆ†ç±»é¢‘æ¬¡.xlsx"
        freq_path = model_dir / freq_filename
        sub.to_excel(freq_path, index=False)

        # ä¸ºæ¯ä¸ªæœºå‹çš„æ‰€æœ‰åˆ†ç±»ç”Ÿæˆä¸€ä¸ªç»¼åˆè¯¦ç»†æ•°æ®æ–‡ä»¶
        detailed_filename = f"{clean_model}_{suffix}_è¯¦ç»†æ•°æ®.xlsx"
        detailed_path = model_dir / detailed_filename
        model_data.to_excel(detailed_path, index=False)
        
        # ç”ŸæˆæŸ±çŠ¶å›¾
        plt.figure(figsize=(12, 6))
        bars = plt.bar(sub["åˆ†ç±»"], sub["æ¬¡æ•°"])
        plt.xticks(rotation=45, ha="right")
        plt.title(f"{model} - {suffix} - åˆ†ç±»é¢‘æ¬¡")
        
        # æ·»åŠ æ•°é‡æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}', ha='center', va='bottom')
        
        plt.tight_layout()
        
        chart_filename = f"{clean_model}_{suffix}_æŸ±çŠ¶å›¾.png"
        chart_path = model_dir / chart_filename
        plt.savefig(chart_path)
        plt.close()
        
        print(f"å·²ç”Ÿæˆ {model} çš„ {suffix} æ•°æ®ï¼Œå…± {len(sub)} ä¸ªåˆ†ç±»ï¼Œ{len(model_data)} æ¡è®°å½•")

        model_summary = {
            "model": model,
            "clean_model": clean_model,
            "suffix": suffix,
            "category_df": sub,
            "chart_path": str(chart_path),
            "total_records": len(model_data)
        }
        summaries.append(model_summary)

    return summaries


# -----------------------------
# ä¿®å¤ generate_analysis_report å‡½æ•°ä¸­çš„åˆ—åé—®é¢˜
# -----------------------------
def generate_analysis_report(df, df_7d, df_non_7d, out_dir, start_date, end_date):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Šå¹¶ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶"""
    report_lines = []

    # 1. è½å…¥Dç­‰çº§äº§å“æ•°æ®ç»Ÿè®¡
    d_grade_data = df[df['å®¡æ ¸åŸå› '] == 'Dç­‰çº§']
    d_grade_count = len(d_grade_data)
    d_grade_models = d_grade_data['æœºå‹åç§°'].unique()
    d_grade_model_count = len(d_grade_models)
    report_lines.append(f"è½å…¥Dç­‰çº§äº§å“æ•°æ®ï¼š{d_grade_count} æ¡")
    report_lines.append(f"è¦†ç›–å‘¨æœŸï¼š{start_date} è‡³ {end_date}")
    report_lines.append(f"æ¶‰åŠæœºå‹ï¼š{', '.join(d_grade_models)}")
    report_lines.append(f"å…±è®¡æœºå‹æ•°é‡ï¼š{d_grade_model_count} æ¬¾\n")

    # 2. å®¡æ ¸åŸå› å æ¯”
    reasons = ["7å¤©æ— ç†ç”±", "15å¤©è´¨é‡æ¢æ–°", "è´¨é‡ç»´ä¿®", "180å¤©åªæ¢ä¸ä¿®"]
    total_count = len(df)
    for reason in reasons:
        count = (df['å®¡æ ¸åŸå› '] == reason).sum()
        percentage = (count / total_count * 100) if total_count > 0 else 0
        report_lines.append(f"å®¡æ ¸åŸå›  - {reason}ï¼š{count} æ¡ï¼Œå æ¯” {percentage:.2f}%")
    report_lines.append("")

    # 3. ä¸ƒå¤©æ— ç†ç”±æœºå‹å æ¯”
    if len(df_7d) > 0:
        model_7d_dist = (
            df_7d['æœºå‹åç§°']
            .value_counts()
            .rename_axis('æœºå‹åç§°')
            .reset_index(name='æ•°é‡')
            .assign(å æ¯”=lambda x: (x['æ•°é‡'] / x['æ•°é‡'].sum() * 100).round(2))
        )
        report_lines.append("ä¸ƒå¤©æ— ç†ç”±æœºå‹å æ¯”ï¼š")
        for _, row in model_7d_dist.iterrows():
            report_lines.append(f"  {row['æœºå‹åç§°']}: {row['æ•°é‡']} æ¡ï¼Œå æ¯” {row['å æ¯”']}%")
        report_lines.append("")

    # 4. éä¸ƒå¤©æ— ç†ç”±æœºå‹å æ¯”
    if len(df_non_7d) > 0:
        model_non_7d_dist = (
            df_non_7d['æœºå‹åç§°']
            .value_counts()
            .rename_axis('æœºå‹åç§°')
            .reset_index(name='æ•°é‡')
            .assign(å æ¯”=lambda x: (x['æ•°é‡'] / x['æ•°é‡'].sum() * 100).round(2))
        )
        report_lines.append("éä¸ƒå¤©æ— ç†ç”±æœºå‹å æ¯”ï¼š")
        for _, row in model_non_7d_dist.iterrows():
            report_lines.append(f"  {row['æœºå‹åç§°']}: {row['æ•°é‡']} æ¡ï¼Œå æ¯” {row['å æ¯”']}%")
        report_lines.append("")

    # 5. æ¯ä¸ªæœºå‹ä¸ƒå¤©æ— ç†ç”±çš„åˆ†ç±»æ•°æ®åˆ†æ
    report_lines.append("æ¯ä¸ªæœºå‹ä¸ƒå¤©æ— ç†ç”±çš„åˆ†ç±»æ•°æ®åˆ†æï¼š")
    for model in df_7d['æœºå‹åç§°'].unique():
        model_data = df_7d[df_7d['æœºå‹åç§°'] == model]
        total_comments = len(model_data)
        no_reason_count = (model_data['åˆ†ç±»'] == 'æ— ç†ç”±é€€è´§').sum()
        no_reason_percentage = (no_reason_count / total_comments * 100) if total_comments > 0 else 0
        top_issues = (
            model_data['åˆ†ç±»']
            .value_counts()
            .reset_index(name='æ¬¡æ•°')
            .rename(columns={'index': 'åˆ†ç±»'})  # ç¡®ä¿åˆ—åæ­£ç¡®
        )

        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å° top_issues åˆ—åå’Œæ•°æ®
        print("ä¸ƒå¤©æ— ç†ç”± - Top Issues:")
        print(top_issues.head())

        top_issues = top_issues[top_issues['æ¬¡æ•°'] >= 2].head(2)
        report_lines.append(f"  {model}:")
        report_lines.append(f"    è¯„è®ºæ€»æ•°ï¼š{total_comments}")
        report_lines.append(f"    æ— ç†ç”±é€€è´§ï¼š{no_reason_count} æ¡ï¼Œå æ¯” {no_reason_percentage:.2f}%")
        for _, row in top_issues.iterrows():
            issue_percentage = (row['æ¬¡æ•°'] / total_comments * 100) if total_comments > 0 else 0
            report_lines.append(f"    Topé—®é¢˜ï¼š{row['åˆ†ç±»']}ï¼Œæ¬¡æ•°ï¼š{row['æ¬¡æ•°']}ï¼Œå æ¯”ï¼š{issue_percentage:.2f}%")
    report_lines.append("")

    # 6. æ¯ä¸ªæœºå‹éä¸ƒå¤©æ— ç†ç”±çš„åˆ†ç±»æ•°æ®åˆ†æ
    report_lines.append("æ¯ä¸ªæœºå‹éä¸ƒå¤©æ— ç†ç”±çš„åˆ†ç±»æ•°æ®åˆ†æï¼š")
    for model in df_non_7d['æœºå‹åç§°'].unique():
        model_data = df_non_7d[df_non_7d['æœºå‹åç§°'] == model]
        total_comments = len(model_data)
        top_issues = (
            model_data['åˆ†ç±»']
            .value_counts()
            .reset_index(name='æ¬¡æ•°')
            .rename(columns={'index': 'åˆ†ç±»'})  # ç¡®ä¿åˆ—åæ­£ç¡®
        )

        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å° top_issues åˆ—åå’Œæ•°æ®
        print("éä¸ƒå¤©æ— ç†ç”± - Top Issues:")
        print(top_issues.head())

        top_issues = top_issues[top_issues['æ¬¡æ•°'] >= 2].head(2)
        report_lines.append(f"  {model}:")
        report_lines.append(f"    æœ‰æ•ˆè¯„è®ºæ€»æ•°ï¼š{total_comments}")
        for _, row in top_issues.iterrows():
            issue_percentage = (row['æ¬¡æ•°'] / total_comments * 100) if total_comments > 0 else 0
            report_lines.append(f"    Topé—®é¢˜ï¼š{row['åˆ†ç±»']}ï¼Œæ¬¡æ•°ï¼š{row['æ¬¡æ•°']}ï¼Œå æ¯”ï¼š{issue_percentage:.2f}%")
    report_lines.append("")

    # 7. æ€»ç»“
    report_lines.append("æ€»ç»“ï¼š")
    report_lines.append(f"æœ¬æ¬¡æŠ¥å‘Šæ—¶é—´è¦†ç›–ï¼š{start_date} è‡³ {end_date}")
    report_lines.append(f"è¦†ç›–æœºå‹ï¼š{', '.join(df['æœºå‹åç§°'].unique())}")
    report_lines.append("éä¸ƒå¤©æ— ç†ç”±åˆ†ç±»ä¸­ï¼Œä»¥ä¸‹æœºå‹çš„é—®é¢˜è¾ƒä¸ºçªå‡ºï¼š")
    for model in df_non_7d['æœºå‹åç§°'].unique():
        model_data = df_non_7d[df_non_7d['æœºå‹åç§°'] == model]
        top_issues = (
            model_data['åˆ†ç±»']
            .value_counts()
            .reset_index(name='æ¬¡æ•°')
            .rename(columns={'index': 'åˆ†ç±»'})  # ç¡®ä¿åˆ—åæ­£ç¡®
        )
        top_issues = top_issues[top_issues['æ¬¡æ•°'] >= 2].head(2)
        for _, row in top_issues.iterrows():
            report_lines.append(f"  {model} - {row['åˆ†ç±»']}ï¼š{row['æ¬¡æ•°']} æ¬¡")

    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_path = out_dir / "åˆ†ææŠ¥å‘Š.txt"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("\n".join(report_lines))

    print(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆï¼š{report_path}")

summary_payload = {
    "start_date": start_date,
    "end_date": end_date,
    "week_range": get_week_workday_range(),
    "coverage_period": determine_coverage_range(df, date_column, start_date, end_date),
    "total_records": len(df),
    "unique_models": sorted(df["æœºå‹åç§°"].dropna().unique().tolist()),
    "reason_stats": summary_df,
    "model_7d_dist": model_7d_dist,
    "model_non_7d_dist": model_non_7d_dist,
    "reason_chart_path": str(reason_chart_path),
    "model_7d_chart_path": str(model_7d_chart_path) if model_7d_chart_path else None,
    "model_non7d_chart_path": str(model_non_7d_chart_path) if model_non_7d_chart_path else None,
}

summaries_7d = build_model_issue_table(df_7d, "7å¤©æ— ç†ç”±", detailed_dir_7d)
summaries_non7d = build_model_issue_table(df_non_7d, "é7å¤©æ— ç†ç”±", detailed_dir_non7d)

model_detail_map: Dict[str, Dict[str, Dict]] = {}

for entry in summaries_7d + summaries_non7d:
    model_name = entry.get("model")
    suffix = entry.get("suffix")
    if model_name not in model_detail_map:
        model_detail_map[model_name] = {}
    model_detail_map[model_name][suffix] = entry

summary_payload["model_details"] = model_detail_map


# åœ¨ä¸»æµç¨‹ä¸­è°ƒç”¨ç”Ÿæˆåˆ†ææŠ¥å‘Šçš„å‡½æ•°
generate_analysis_report(df, df_7d, df_non_7d, out_dir, start_date, end_date)

if args.generate_ppt:
    ppt_path = Path(args.ppt_path) if args.ppt_path else (out_dir / DEFAULT_PPT_PATH)
    llm_params = {
        "timeout": args.llm_timeout,
        "top_n": args.llm_top_n,
        "coverage": args.llm_coverage,
        "focus": args.llm_focus
    }
    try:
        generate_ppt(
            summary_payload,
            ppt_path,
            Path(args.ppt_template) if args.ppt_template else None,
            use_llm=args.use_llm,
            llm_params=llm_params
        )
    except (IOError, OSError) as exc:
        print(f"ç”ŸæˆPPTå¤±è´¥ï¼ˆæ–‡ä»¶IOé”™è¯¯ï¼‰: {exc}")
    except LLMGenerationError as exc:
        print(f"ç”ŸæˆPPTå¤±è´¥ï¼ˆLLMè°ƒç”¨é”™è¯¯ï¼‰: {exc}")
    except Exception as exc:
        print(f"ç”ŸæˆPPTå¤±è´¥ï¼ˆæœªçŸ¥é”™è¯¯ï¼‰: {exc}")
        import traceback
        traceback.print_exc()

print("âœ… æ‰€æœ‰å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° output ç›®å½•ï¼")
print("æ–‡ä»¶ç»“æ„ï¼š")
print("output/")
print("â”œâ”€â”€ å®¡æ ¸åŸå› ç»Ÿè®¡.xlsx")
print("â”œâ”€â”€ 7å¤©æ— ç†ç”±_æœºå‹åˆ†å¸ƒ.xlsx")
print("â”œâ”€â”€ é7å¤©æ— ç†ç”±_æœºå‹åˆ†å¸ƒ.xlsx")
print("â”œâ”€â”€ å®¡æ ¸åŸå› å æ¯”.png")
print("â”œâ”€â”€ 7å¤©æ— ç†ç”±_æœºå‹åˆ†å¸ƒ.png")
print("â”œâ”€â”€ é7å¤©æ— ç†ç”±_æœºå‹åˆ†å¸ƒ.png")
print("â””â”€â”€ è¯¦ç»†æ•°æ®/")
print("    â”œâ”€â”€ 7å¤©æ— ç†ç”±/")
print("    â”‚   â””â”€â”€ [æœºå‹åç§°]/")
print("    â”‚       â”œâ”€â”€ [æœºå‹]_7å¤©æ— ç†ç”±_åˆ†ç±»é¢‘æ¬¡.xlsx")
print("    â”‚       â”œâ”€â”€ [æœºå‹]_7å¤©æ— ç†ç”±_æŸ±çŠ¶å›¾.png")
print("    â”‚       â””â”€â”€ [æœºå‹]_7å¤©æ— ç†ç”±_è¯¦ç»†æ•°æ®.xlsx")
print("    â””â”€â”€ é7å¤©æ— ç†ç”±/")
print("        â””â”€â”€ [æœºå‹åç§°]/")
print("            â”œâ”€â”€ [æœºå‹]_é7å¤©æ— ç†ç”±_åˆ†ç±»é¢‘æ¬¡.xlsx")
print("            â”œâ”€â”€ [æœºå‹]_é7å¤©æ— ç†ç”±_æŸ±çŠ¶å›¾.png")
print("            â””â”€â”€ [æœºå‹]_é7å¤©æ— ç†ç”±_è¯¦ç»†æ•°æ®.xlsx")
if args.generate_ppt:
    print("â”œâ”€â”€ report.pptx")
